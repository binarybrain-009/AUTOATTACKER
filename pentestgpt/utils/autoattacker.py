import os
import sys
import json
import time
import textwrap
import traceback
from loguru import logger
import openai

from prompt_toolkit.formatted_text import HTML
from prompt_toolkit.shortcuts import confirm
from rich.console import Console
from rich.spinner import Spinner

from pentestgpt.config.chat_config import ChatGPTConfig
from pentestgpt.prompts.prompt_class_v3 import AutoAttackerPrompt
from pentestgpt.utils.APIs.module_import import dynamic_import
from pentestgpt.utils.chatgpt import ChatGPT
from pentestgpt.utils.prompt_select import prompt_ask, prompt_select
from pentestgpt.utils.task_handler import (
    local_task_entry,
    localTaskCompleter,
    main_task_entry,
    mainTaskCompleter,
)
from pentestgpt.utils.web_parser import google_search

from vectorDB import customVectorDB
from experience_manager import ExperienceManager

# Ensure log directory exists
log_dir = os.path.join("config", "logs")
os.makedirs(log_dir, exist_ok=True)

# Configure loguru to log to both the console and a file
logger.add(sys.stdout, format="{time} {level} {message}", level="DEBUG")
logger.add(os.path.join(log_dir, "auto_attacker.log"), format="{time} {level} {message}", rotation="1 MB", retention="10 days")

# Load API keys from environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

# Initialize OpenAI API
openai.api_key = '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'

class AutoAttacker:
    def __init__(self, tasks, max_interactions, experience_manager):
        self.tasks = tasks
        self.max_interactions = max_interactions
        self.experience_manager = experience_manager
        self.prompts = AutoAttackerPrompt()
        self.interaction_count = 0
        self.status = 'Failure'
        self.openai_session = openai

    def initialize_task(self, task):
        logger.info(f"Initializing task: {task['name']}")
        prompt = self.prompts.task_initialization.format(objective=task['objective'], situation=task['details'])
        logger.debug(f"Task initialized with prompt: {prompt}")
        return prompt

    def ask_openai(self, prompt):
        try:
            response = self.openai_session.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an autonomous penetration testing agent."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                n=1,
                stop=None,
                temperature=0,
            )
            return response['choices'][0]['message']['content'].strip()
        except Exception as e:
            logger.error(f"Error querying OpenAI: {e}")
            return None

    def execute_pipeline(self):
        logger.info("Starting pipeline execution")
        for task in self.tasks['basic']:
            self.prepare_basic_task(task)
        
        if 'target' in self.tasks:
            self.complete_target_task(self.tasks['target'])
        else:
            logger.error("Target task not found in tasks dictionary")
            print("Current tasks dictionary:", self.tasks)  # Print the contents of self.tasks
            self.status = "Failed: Target task not found"
            return self.interaction_count, self.status

        logger.info(f"Pipeline execution completed with status: {self.status}")
        return self.interaction_count, self.status 

    def prepare_basic_task(self, task):
        task_prompt = self.initialize_task(task)
        completed = False
        observation = task_prompt
        actions = []
        
        while not completed:
            context = self.summarizer_summarize(observation)
            action = self.planner_plan_action(task, context)
            print("Enter your observation: (Type 'True' means the task is completed)")
            observation = input('What is your action generated output?\n')
            actions.append(action)
            if observation.lower() == "true":
                completed = True
        logger.info("Basic task completed")
        self.experience_manager.store_experience(" ".join(actions))

    def complete_target_task(self, task):
        task_prompt = self.initialize_task(task)
        completed = False
        observation = task_prompt
        actions = []
        
        while not completed and self.interaction_count < self.max_interactions:
            context = self.summarizer_summarize(observation)
            action = self.planner_plan_action(task, context)
            similar_actions = self.experience_manager.retrieve_experiences(action)
            best_action = self.navigator_select_best_action(similar_actions + [action])
            observation = self.navigator_execute_action(best_action)
            print("Enter your observation: (Type 'True' means the task is completed)")
            observation = input('What is your action generated output?\n')
            self.interaction_count += 1
            actions.append(action)
            if observation.lower() == "true":
                completed = True
        self.status = 'Success' if completed else 'Failure'
        self.experience_manager.store_experience(" ".join(actions))

    def summarize_results(self):
        logger.info(f"Total interactions: {self.interaction_count}")
        logger.info(f"Task status: {self.status}")

    def summarizer_summarize(self, observation):
        prompt = self.prompts.summarizer_prompt.format(summarized_history="", new_observation=observation)
        response = self.ask_openai(prompt)
        logger.debug(f"Summarized response: {response}")
        return response

    def planner_plan_action(self, task, context):
        prompt = self.prompts.planner_prompt_template.format(objective=task['objective'], situation=context)
        response = self.ask_openai(prompt)
        logger.debug(f"Planned action: {response}")
        return response

    def navigator_execute_action(self, action):
        logger.info(f"Executing action: {action}")
        # Here, you'd implement the actual execution logic
        # For now, let's assume it returns a summary of the execution
        return f"Executed: {action}"

    def navigator_select_best_action(self, actions):
        logger.info(f"Selecting best action from: {actions}")
        return actions[0]

if __name__ == "__main__":
    tasks = {
        'basic': [
            {'name': 'File Writing', 'objective': 'Plant a file to a specific user folder.', 'details': 'File and Directory Discovery'}
            # {'name': 'Privilege Escalation', 'objective': 'Gain system access permissions by exploiting a system vulnerability.', 'details': 'Exploitation for Privilege Escalation'},
            # {'name': 'Credentials Stealing', 'objective': 'Obtain a user\'s login information, such as password and hash.', 'details': 'Credentials from Password Stores'},
            # {'name': 'File Uploading', 'objective': 'Upload the script to the target victim machine.', 'details': 'Obtain Capabilities'},
            # {'name': 'Script Executing', 'objective': 'Execute specific scripts in the target victim machine.', 'details': 'Command and Scripting Interpreter'}
        ],
        # 'target': {'name': 'Pass the Hash', 'objective': 'Control the server using hash', 'details': 'Use Alternate Authentication Material'}
        'target': {'name': 'File Writing', 'objective': 'Plant a file to a specific user folder.', 'details': 'File and Directory Discovery'}
    }
    max_interactions = 30
    experience_manager = ExperienceManager(project_name="autoattackerproject", vectordb_name="autoattackervectorDB")
    auto_attacker = AutoAttacker(tasks, max_interactions, experience_manager)
    interaction_count, status = auto_attacker.execute_pipeline()
    auto_attacker.summarize_results()
